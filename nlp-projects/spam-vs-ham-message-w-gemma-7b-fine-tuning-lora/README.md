## Spam vs Ham Message w/ Gemma 7B Fine-Tuning (LoRA)

(kaggle link -> https://www.kaggle.com/code/banddaniel/spam-vs-ham-message-w-gemma-7b-fine-tuning-lora)

<i><span style="color:#e74c3c;"><b>MAIN GOAL: </b>  I tried to classify spam or ham message with fine-tuned Gemma 7B model using prompts.</span></i>


* I applied several <b>preprocessing</b> operations (cleaning,dropping stop words etc.),
* I created Gemma prompts,
* I have modified this notebook [1],
* An end-2-end test prediction pipeline function,

## Test Predictions

<img width="630" alt="Screenshot 2024-03-21 at 11 12 42 PM" src="https://github.com/john-fante/my-deep-learning-projects/assets/50263592/a24fc163-a750-452a-8ed1-1ffe1c80b8a0">
<br>
<img width="606" alt="Screenshot 2024-03-21 at 11 12 50 PM" src="https://github.com/john-fante/my-deep-learning-projects/assets/50263592/a5a5e724-32ca-45d7-8263-398ea577619f">


## My Another Projects
* [News Zero-Shot Topic Modelling w/BERTopic](https://www.kaggle.com/code/banddaniel/news-zero-shot-topic-modelling-w-bertopic)
* [Complaint Analysis w/Ensemble Model (CatBoost, LR)](https://www.kaggle.com/code/banddaniel/complaint-analysis-w-ensemble-model-catboost-lr)
* [Gemma 2B Text Summarization w/Zero-Shot Prompting](https://www.kaggle.com/code/banddaniel/gemma-2b-text-summarization-w-zero-shot-prompting)



## References
1. https://ai.google.dev/gemma/docs/lora_tuning
