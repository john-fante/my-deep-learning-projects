## Manufacturing Question-Answer w/Gemma 7B (Fine-Tuning LoRA)

(kaggle link -> https://www.kaggle.com/code/banddaniel/manufacturing-question-answer-w-gemma-7b-lora)

<i><span style="color:#e74c3c;"><b>MAIN GOAL: </b>  I tried to an answer and question project with fine-tuned Gemma 7B model using prompts.</span></i>


* I applied several <b>preprocessing</b> operations (cleaning,dropping stop words etc.),
* I created <b>Gemma prompts</b>,
* I fine-tuned a Gemma 7B model with <b>LoRA</b>,
* I have modified this notebook [1],
* An end-2-end test prediction pipeline function,
* Test evaluation metrics <b>(mean cosine similarity, mean BLEU Score)</b>,


## Test Predictions
![Screenshot 2024-04-02 at 4 24 00 PM-3-2](https://github.com/john-fante/my-deep-learning-projects/assets/50263592/2b4f5a7d-b390-4452-95ee-9663e96f776b)


## My Another Projects
* [News Zero-Shot Topic Modelling w/BERTopic](https://www.kaggle.com/code/banddaniel/news-zero-shot-topic-modelling-w-bertopic)
* [Complaint Analysis w/Ensemble Model (CatBoost, LR)](https://www.kaggle.com/code/banddaniel/complaint-analysis-w-ensemble-model-catboost-lr)


## References
1. https://ai.google.dev/gemma/docs/lora_tuning
